{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBeHMc0LoAzcRE7dVYRTMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsleweon11/Enorme-Strategist-Group-Machine-learning-and-Lead-Gen-Case-Study/blob/main/Enorme_Strategist_Group_Machine_learning_and_Lead_Gen_Case_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGMJdSjiixYP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Style for plots\n",
        "sns.set(style=\"darkgrid\")\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "# Load data files\n",
        "zombie_properties_df = pd.read_csv('/path/to/Zombie Properties 2024-05-16.csv')\n",
        "propensity_to_default_df = pd.read_csv('/path/to/Propensity to Default By Zip Code 2024-05-16.csv')\n",
        "mn_cleaned_phone_df = pd.read_csv('/path/to/MN Cleaned Cell and Business phone 90 day Auction-5-15-24 - cleaned_combined_MN90DaysAllcounties_Cleaned.csv')\n",
        "rdc_inventory_hotness_df = pd.read_csv('/path/to/RDC_Inventory_Hotness_Metrics_Metro_History.csv')\n",
        "metro_invt_fs_uc_sfrcondo_df = pd.read_csv('/path/to/Metro_invt_fs_uc_sfrcondo_sm_month - Metro_invt_fs_uc_sfrcondo_sm_month.csv')\n",
        "cleaned_foreclosure_df = pd.read_csv('/path/to/cleaned_foreclosure_data.csv')\n",
        "\n",
        "# Ensure all dates are in the same format\n",
        "cleaned_foreclosure_df['AuctionDate'] = pd.to_datetime(cleaned_foreclosure_df['AuctionDate']).dt.tz_localize(None)\n",
        "mn_cleaned_phone_df['AuctionDate'] = pd.to_datetime(mn_cleaned_phone_df['AuctionDate']).dt.tz_localize(None)\n",
        "\n",
        "# Merging datasets\n",
        "merged_df1 = pd.merge(cleaned_foreclosure_df, zombie_properties_df, how='left', left_on='StateCode', right_on='SitusState')\n",
        "merged_df2 = pd.merge(merged_df1, propensity_to_default_df, how='left', left_on='Zipcode', right_on='SitusZip')\n",
        "merged_df3 = pd.merge(merged_df2, mn_cleaned_phone_df, how='left', left_on=['AuctionDate', 'CountyName_x'], right_on=['AuctionDate', 'County'])\n",
        "merged_df4 = pd.merge(merged_df3, rdc_inventory_hotness_df, how='left', left_on=['StateCode', 'CountyName'], right_on=['StateName', 'cbsa_title'])\n",
        "merged_df5 = pd.merge(merged_df4, metro_invt_fs_uc_sfrcondo_df, how='left', left_on=['StateCode', 'CountyName'], right_on=['StateName', 'RegionName'])\n",
        "\n",
        "# Handle missing values\n",
        "merged_df5.fillna(0, inplace=True)\n",
        "\n",
        "# Save the final merged dataframe to a CSV file\n",
        "merged_df5.to_csv('/path/to/final_merged_foreclosure_data.csv', index=False)\n",
        "\n",
        "# Load the final cleaned data\n",
        "df = pd.read_csv('/path/to/final_merged_foreclosure_data.csv')\n",
        "\n",
        "# Feature Engineering: Example of creating a binary target variable\n",
        "# Assuming we are predicting if a property will be a potential deal\n",
        "df['Potential_Deal'] = np.where(df['REOs'] > 0, 1, 0)\n",
        "\n",
        "# Select features and target variable\n",
        "features = df.drop(['Potential_Deal', 'StateCode', 'CountyName', 'Zipcode'], axis=1)\n",
        "target = df['Potential_Deal']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Detailed classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "feature_importances = rf_model.feature_importances_\n",
        "features = X_train.columns\n",
        "importances = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
        "importances = importances.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Plot Feature Importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=importances, palette='Blues_r')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()\n",
        "\n",
        "# Save the model\n",
        "import joblib\n",
        "joblib.dump(rf_model, '/path/to/rf_model.pkl')\n",
        "\n",
        "# Save code to Google Colab\n",
        "with open('/path/to/Enorme_Strategic_Group_Real_Estate_Case_Study.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Style for plots\n",
        "sns.set(style=\"darkgrid\")\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "# Load data files\n",
        "zombie_properties_df = pd.read_csv('/path/to/Zombie Properties 2024-05-16.csv')\n",
        "propensity_to_default_df = pd.read_csv('/path/to/Propensity to Default By Zip Code 2024-05-16.csv')\n",
        "mn_cleaned_phone_df = pd.read_csv('/path/to/MN Cleaned Cell and Business phone 90 day Auction-5-15-24 - cleaned_combined_MN90DaysAllcounties_Cleaned.csv')\n",
        "rdc_inventory_hotness_df = pd.read_csv('/path/to/RDC_Inventory_Hotness_Metrics_Metro_History.csv')\n",
        "metro_invt_fs_uc_sfrcondo_df = pd.read_csv('/path/to/Metro_invt_fs_uc_sfrcondo_sm_month - Metro_invt_fs_uc_sfrcondo_sm_month.csv')\n",
        "cleaned_foreclosure_df = pd.read_csv('/path/to/cleaned_foreclosure_data.csv')\n",
        "\n",
        "# Ensure all dates are in the same format\n",
        "cleaned_foreclosure_df['AuctionDate'] = pd.to_datetime(cleaned_foreclosure_df['AuctionDate']).dt.tz_localize(None)\n",
        "mn_cleaned_phone_df['AuctionDate'] = pd.to_datetime(mn_cleaned_phone_df['AuctionDate']).dt.tz_localize(None)\n",
        "\n",
        "# Merging datasets\n",
        "merged_df1 = pd.merge(cleaned_foreclosure_df, zombie_properties_df, how='left', left_on='StateCode', right_on='SitusState')\n",
        "merged_df2 = pd.merge(merged_df1, propensity_to_default_df, how='left', left_on='Zipcode', right_on='SitusZip')\n",
        "merged_df3 = pd.merge(merged_df2, mn_cleaned_phone_df, how='left', left_on=['AuctionDate', 'CountyName_x'], right_on=['AuctionDate', 'County'])\n",
        "merged_df4 = pd.merge(merged_df3, rdc_inventory_hotness_df, how='left', left_on=['StateCode', 'CountyName'], right_on=['StateName', 'cbsa_title'])\n",
        "merged_df5 = pd.merge(merged_df4, metro_invt_fs_uc_sfrcondo_df, how='left', left_on=['StateCode', 'CountyName'], right_on=['StateName', 'RegionName'])\n",
        "\n",
        "# Handle missing values\n",
        "merged_df5.fillna(0, inplace=True)\n",
        "\n",
        "# Save the final merged dataframe to a CSV file\n",
        "merged_df5.to_csv('/path/to/final_merged_foreclosure_data.csv', index=False)\n",
        "\n",
        "# Load the final cleaned data\n",
        "df = pd.read_csv('/path/to/final_merged_foreclosure_data.csv')\n",
        "\n",
        "# Feature Engineering: Example of creating a binary target variable\n",
        "# Assuming we are predicting if a property will be a potential deal\n",
        "df['Potential_Deal'] = np.where(df['REOs'] > 0, 1, 0)\n",
        "\n",
        "# Select features and target variable\n",
        "features = df.drop(['Potential_Deal', 'StateCode', 'CountyName', 'Zipcode'], axis=1)\n",
        "target = df['Potential_Deal']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Detailed classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "feature_importances = rf_model.feature_importances_\n",
        "features = X_train.columns\n",
        "importances = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
        "importances = importances.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Plot Feature Importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=importances, palette='Blues_r')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()\n",
        "\n",
        "# Save the model\n",
        "import joblib\n",
        "joblib.dump(rf_model, '/path/to/rf_model.pkl')\n",
        "\"\"\")\n"
      ]
    }
  ]
}